export NETOP_NETWORK_TYPE="HostDeviceNetwork"       # network type
export NETOP_NETWORK_NAME="hostdev-rdma-device"     # secondary network base name
export NETOP_RESOURCE="rdma_device"		    # resource base name
export NETOP_RESOURCE_PATH="nvidia.com"             # resource path for resource request
export NETOP_NETWORK_POOL="hostdev-pool"            # nvidia ipam pool name
export NETOP_NETWORK_VLAN="0"                       # vlan id
export NETOP_VENDOR="15b3"                          # NVidia vendor id
#
# define IP pools per network
# 
# NETWORK_RANGE      IP pool for your secondary rdma network. different from the K8S CDIR
# NETWORK_GW         gateway IP for your secondary rdma network for your cluster
# PERNODE_BLOCKSIZE  number of IP addrs that can be allocated per worker node in your cluster from the nvipam pool
# NIDX,NETWORK_RANGE,NETWORK_GW,PERNODE_BLOCKSIZE
declare -A NETOP_IPPOOLS # associative array
case "${IPAM_POOL_TYPE}" in
IPPool)
  su_1=( "a,192.169.0.0/16,192.169.0.1,32" "b,192.170.0.0/16,192.170.0.1,32" )
  su_2=( "a,192.171.0.0/16,192.171.0.1,32" "b,192.172.0.0/16,192.172.0.1,32" )
  ;;
CIDRPool)
  su_1=( "a,192.169.0.0/16,1,24" "b,192.170.0.0/16,1,24" )
  su_2=( "a,192.171.0.0/16,1,24" "b,192.172.0.0/16,1,24" )
  ;;
esac
NETOP_IPPOOLS["su-1"]=${su_1[@]}
NETOP_IPPOOLS["su-2"]=${su_2[@]}
export NET_IPPOOLS
export NETOP_SULIST=( su-1 su-2 ) # SU (scalable unit) list, different IPPools for diffent pod sets
# devices index, device PCI id, HCAMAX (RDMA shared only), list of network device | PCI BFDs
export NETOP_NETLIST=( a,,,0000:07:00.0,0000:08:00.0 b,,,0000:09:00.0,0000:0a:00.0 )
