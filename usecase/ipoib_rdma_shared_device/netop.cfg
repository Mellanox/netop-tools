export NETOP_NETWORK_TYPE="IPoIBNetwork"              # network type
export NETOP_NETWORK_NAME="ipoib-rdma-shared-device"  # secondary network base name
export NETOP_RESOURCE="rdma_shared_device"            # resource base name
export NETOP_RESOURCE_PATH="rdma"                     # resource path for resource request
export NETOP_NETWORK_POOL="ipoib-pool"                # nvidia ipam pool name
export NETOP_NETWORK_VLAN="0"                         # vlan id
export NETOP_VENDOR="15b3"                            # NVidia vendor id
export NETOP_HCAMAX="63"                              # max shared device count
# devices index, device PCI id, HCAMAX (RDMA shared only), list of network device | PCI BFDs
#export NETOP_NETLIST=( a,101b,${NETOP_HCAMAX},ens1f0np0 b,101b,${NETOP_HCAMAX},ens1f1np1 c,101b,${NETOP_HCAMAX},ens2np0 )
#export NETOP_NETLIST=( a,,,ens1f0np0 b,,,ens1f1np1 c,,,ens2np0 )
#export NETOP_NETLIST=( a,,60,ens1f0np0 )
#
# define IP ppools per network
# 
# NETWORK_RANGE      IP pool for your secondary rdma network. different from the K8S CDIR
# NETWORK_GW         gateway IP for your secondary rdma network for your cluster
# PERNODE_BLOCKSIZE  number of IP addrs that can be allocated per worker node in your cluster from the nvipam pool
# NIDX,NETWORK_RANGE,NETWORK_GW,PERNODE_BLOCKSIZE
declare -A NETOP_IPPOOLS # associative array
case "${NVIPAM_POOL_TYPE}" in
IPPool)
  su_1=( "a,192.169.0.0/16,192.169.0.1,32" "b,192.170.0.0/16,192.170.0.1,32" )
# su_2=( "a,192.171.0.0/16,192.171.0.1,32" "b,192.172.0.0/16,192.172.0.1,32" )
  ;;
CIDRPool)
  su_1=( "a,192.169.0.0/16,1,24" "b,192.170.0.0/16,1,24" )
# su_2=( "a,192.171.0.0/16,1,24" "b,192.172.0.0/16,1,24" )
  ;;
esac
NETOP_IPPOOLS["su-1"]=${su_1[@]}
#NETOP_IPPOOLS["su-2"]=${su_2[@]}
export NET_IPPOOLS
export NETOP_SULIST=( su-1 ) # SU (scalable unit) list, different IPPools for diffent pod sets
if [ ! -v NETOP_NETLIST ];then
  export NETOP_NETLIST=( a,,60,ibs0f0 b,,60,ibs0f1 )
fi
