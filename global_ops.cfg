# Edit the file by setting additional variable <USECASE> at the end:
# 
#export NETOP_ROOT_DIR="/local/netop-tools"
#export NETOP_ROOT_DIR=$(git rev-parse --show-toplevel)
if [ -z ${NETOP_ROOT_DIR} ];then
    echo "Variable NETOP_ROOT_DIR must be set in global_ops.cfg file"
    exit 1
fi

# To allow overwrite default configuration options with chaning Git configuration
if [ ! -v GLOBAL_OPS_USER ];then
  export GLOBAL_OPS_USER=${NETOP_ROOT_DIR}/global_ops_user.cfg
fi

if [ -r ${GLOBAL_OPS_USER} ];then
  echo "Reading user configuration file: ${GLOBAL_OPS_USER}"
  source ${GLOBAL_OPS_USER}
else
  echo "ERROR: User configuration file: ${GLOBAL_OPS_USER} not found"
  echo "Be sure using FULL PATH for GLOBAL_OPS_USER"
  echo "Current directory: $(pwd)"
  exit 1
fi

export K8CIDR="192.168.0.0/16"                      # this is your k8s cluster CIDR
export K8SVER=${K8SVER:-"1.29"}                     # select the required K8S version
export K8CL=${K8CL:-"kubectl"}                      # define the k8s commandline tool
export HOST_OS=${HOST_OS:-"ubuntu"}                 # select the host OS
export NETOP_NAMESPACE=${NETOP_NAMESPACE:-"nvidia-network-operator"}    # set the nvidia network operator name space. may differ in yoyur cluster
export NETOP_APP_NAMESPACES=( "default" ) # set the name spaces that your application pods will run in. may vary in your cluster
export NETOP_NETWORK_RANGE="192.169.0.0/16"         # define the IP pool for your secondary rdma network. different from the K8S CDIR
export NETOP_NETWORK_GW="192.169.0.1"               # define the gateway IP for your secondary rdma network for your cluster
export NETOP_PERNODE_BLOCKSIZE="32"                 # define the number of IP addrs that can be allocated per worker node
                                                    # inyour cluster from the nvipam pool
export PROD_VER=1 # is this a production version
if [ "${PROD_VER}" = "1" ];then
# export NETOP_VERSION="24.7.0"                     # select network operator version
# export NETOP_VERSION="24.10.0"                    # select network operator version
  export NETOP_VERSION=${NETOP_VERSION:-"25.1.0"}   # select network operator version
# export NETOP_HELM_URL="https://helm.ngc.nvidia.com/nvidia/cloud-native/charts/network-operator-${NETOP_VERSION}.tgz"
  export HELM_NVIDIA_REPO="https://helm.ngc.nvidia.com/nvidia"     # helm repo URL
  export NETOP_HELM_URL="https://helm.ngc.nvidia.com/nvidia/charts/network-operator-${NETOP_VERSION}.tgz"  # helm chart tarball
else
  export NETOP_VERSION="24.7.0-rc.2"               # select network operator version
  export HELM_NVIDIA_REPO="https://helm.ngc.nvidia.com/nvstaging"  # helm repo URL
  export NETOP_HELM_URL="https://helm.ngc.nvidia.com/nvstaging/mellanox/charts/network-operator-${NETOP_VERSION}.tgz" # helm chart tarball
  export NGC_API_KEY=`cat /root/.ngc/config|grep apikey|cut -d' ' -f3` # dev NGC_API_KEY
  export NGC_SECRET="ngc-image-secret"                                 # dev image secret
fi

export CALICO_ROOT=${CALICO_ROOT:-"3.28.2"}
export CALICO_VERSION="v${CALICO_ROOT}"

export CNIPLUGINS_VERSION=${CNIPLUGINS_VERSION:-"v1.5.1"}

#
# select the use case to implement
#
# Replace with a different usecase scenario if necessary
# Usecase scenarios are named after a folder names under 'usecase' directory
export USECASE=${USECASE:-"hostdev_rdma_sriov"}
#export USECASE=${USECASE:-"ipoib_rdma_shared_device"}
#export USECASE=${USECASE:-"macvlan_rdma_shared_device"}
#export USECASE=${USECASE:-"sriovibnet_rdma"}
#export USECASE=${USECASE:-"sriovnet_rdma"}

# default number of VFs to define
case ${USECASE} in
hostdev_rdma_sriov|sriovibnet_rdma|sriovnet_rdma)
  export NUM_VFS=${NUM_VFS:-"8"}
  ;;
ipoib_rdma_shared_device|macvlan_rdma_shared_device)
  export NUM_VFS=${NUM_VFS:-"0"}
  ;;
esac
if [ ! -v DEVICE_TYPES ];then
DEVICE_TYPES=( "connectx-6" )
fi
#
# select the ipam mode.
# nv-ipam: preferred scaleable > 1000's
# whereabouts small clusters < 60 nodes
#

# https://github.com/Mellanox/nvidia-k8s-ipam
export IPAM_TYPE=${IPAM_TYPE:-"nv-ipam"}
#export NVIPAM_POOL_TYPE="CIDRPool"
export NVIPAM_POOL_TYPE=${NVIPAM_POOL_TYPE:-"IPPool"}
#export IPAM_TYPE="whereabouts"
#export IPAM_TYPE="dhcp"

case "${NVIPAM_POOL_TYPE}" in
IPPool)
  if [ ! -v su_1 ];then
    su_1=( "a,192.169.0.0/16,192.169.0.1,32" "b,192.170.0.0/16,192.170.0.1,32" )
  fi
  if [ ! -v su_2 ];then
    su_2=( "a,192.171.0.0/16,192.171.0.1,32" "b,192.172.0.0/16,192.172.0.1,32" )
  fi
  ;;
CIDRPool)
  if [ ! -v su_1 ];then
    su_1=( "a,192.169.0.0/16,1,24" "b,192.170.0.0/16,1,24" )
  fi
#  if [ ! -v su_2 ];then
# su_2=( "a,192.171.0.0/16,1,24" "b,192.172.0.0/16,1,24" )
  ;;
esac

#
# set to false when running host mofed, not container version
#
OFED_ENABLE=${OFED_ENABLE:-"true"}
#
# set true to fix OFED_BLACKLIST bug
#
OFED_BLACKLIST_ENABLE=${OFED_BLACKLIST_ENABLE:-"false"}
#
# disable when GPU-operator is running NFD
#
NFD_ENABLE=${NFD_ENABLE:-"true"}
#
# set true when only want the config yaml files created
#
CREATE_CONFIG_ONLY=${CREATE_CONFIG_ONLY:-"1"}

if [ "${CREATE_CONFIG_ONLY}" = "1" ];then
  docmd="echo Install command: "
else
  docmd=""
fi
#
# for RDMA AI E/W traffic, 9000 is usally the selected MTU number
#
NETOP_MTU=${MTU_DEFAULT:-"1500"}
# Use shared or exclusive mode for rdma namespace.
# When set to false, only allocated RDMA devices will be visible on the pod
# When set to true, all RDMA devices will be visible on the pod
RDMASHAREDMODE=${RDMASHAREDMODE:-"true"}
#
# use when setting Source Based Routing
# loads the SBR plugin for the network definition
#
SBRMODE=${SBRMODE:-"false"}

source ${NETOP_ROOT_DIR}/usecase/${USECASE}/netop.cfg
