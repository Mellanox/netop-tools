# Edit the file by setting additional variable <USECASE> at the end:
# 
#export NETOP_ROOT_DIR="/local/netop-tools"
#export NETOP_ROOT_DIR=$(git rev-parse --show-toplevel)
if [ -z ${NETOP_ROOT_DIR} ];then
    echo "Variable NETOP_ROOT_DIR must be set in global_ops.cfg file"
    exit 1
fi

# To allow overwrite default configuration options with chaning Git configuration
if [ ! -v GLOBAL_OPS_USER ];then
  export GLOBAL_OPS_USER=${NETOP_ROOT_DIR}/global_ops_user.cfg
fi

if [ -r ${GLOBAL_OPS_USER} ];then
  echo "Reading user configuration file: ${GLOBAL_OPS_USER}"
  source ${GLOBAL_OPS_USER}
else
  echo "ERROR: User configuration file: ${GLOBAL_OPS_USER} not found"
  echo "Be sure using FULL PATH for GLOBAL_OPS_USER"
  echo "Current directory: $(pwd)"
  exit 1
fi

export K8CIDR="192.168.0.0/16"                      # this is your k8s cluster CIDR
export K8SVER=${K8SVER:-"1.29"}                     # select the required K8S version
export K8CL=${K8CL:-"kubectl"}                      # define the k8s commandline tool
export HOST_OS=${HOST_OS:-"ubuntu"}                 # select the host OS
export NETOP_NAMESPACE=${NETOP_NAMESPACE:-"nvidia-network-operator"}    # set the nvidia network operator name space. may differ in yoyur cluster
if [ ! -v NETOP_APP_NAMESPACES ];then
  export NETOP_APP_NAMESPACES=( "default" ) # set the name spaces that your application pods will run in. may vary in your cluster
fi
#
# setting to force MACADDR sourcing from only owning interface
# set in global_ops_user.cfg if worker node arp config returning wrong MACADDR
# SYSCTL_CONFIG="net.ipv4.conf.all.rp_filter=0,net.ipv4.conf.all.arp_announce=2,net.ipv4.conf.all.arp_ignore=1"
export SYSCTL_CONFIG=${SYSCTL_CONFIG:-""}           # overide ARP config internal to test pod
export NETOP_NETWORK_RANGE=${NETOP_NETWORK_RANGE:-"192.170.0.0/16"} # define the IP pool for your secondary rdma network. different from the K8S CDIR
                                                    # because the 2ndary E/W RDMA networks are implemented as L2 they are NOT routed.
export NETOP_NETWORK_START=${NETOP_NETWORK_START:-""} # start of IP pool range, optional
export NETOP_NETWORK_END=${NETOP_NETWORK_END:-""}   # end of IP pool range, optional
export NETOP_NETWORK_ROUTE=${NETOP_NETWORK_ROUTE:-""}   # subnet route, optional
export NETOP_NETWORK_GW=${NETOP_NETWORK_GW:-""}     # define the gateway IP for your secondary rdma network for your cluster
export NETOP_NETWORK_EXCLUDE=${NETOP_NETWORK_EXCLUDE:-""}   # whereabouts list of excluded IP addresses
export NETOP_PERNODE_BLOCKSIZE=${NETOP_PERNODE_BLOCKSIZE:-"32"} # define the number of IP addrs that can be allocated per worker node
                                                    # in your cluster from the nvipam pool
export NETOP_VENDOR="15b3"                          # NVidia vendor id
export NETOP_SULIST=( "su-1" )                      # SU (scalable unit) list, different IPPools for diffent pod sets

export PROD_VER=1 # is this a production version
if [ "${PROD_VER}" = "1" ];then
# export NETOP_VERSION=${NETOP_VERSION:-"25.1.0"}   # select network operator version
  export NETOP_VERSION=${NETOP_VERSION:-"25.4.0"}   # select network operator version
# export NETOP_HELM_URL="https://helm.ngc.nvidia.com/nvidia/cloud-native/charts/network-operator-${NETOP_VERSION}.tgz"
  export HELM_NVIDIA_REPO="https://helm.ngc.nvidia.com/nvidia"     # helm repo URL
  export NETOP_HELM_URL="https://helm.ngc.nvidia.com/nvidia/charts/network-operator-${NETOP_VERSION}.tgz"  # helm chart tarball
else
  export NETOP_VERSION="24.7.0-rc.2"               # select network operator version
  export HELM_NVIDIA_REPO="https://helm.ngc.nvidia.com/nvstaging"  # helm repo URL
  export NETOP_HELM_URL="https://helm.ngc.nvidia.com/nvstaging/mellanox/charts/network-operator-${NETOP_VERSION}.tgz" # helm chart tarball
  export NGC_API_KEY=`cat /root/.ngc/config|grep apikey|cut -d' ' -f3` # dev NGC_API_KEY
  export NGC_SECRET="ngc-image-secret"                                 # dev image secret
fi

export CALICO_ROOT=${CALICO_ROOT:-"3.28.2"}
export CALICO_VERSION="v${CALICO_ROOT}"

export CNIPLUGINS_VERSION=${CNIPLUGINS_VERSION:-"v1.5.1"}

#
# select the use case to implement
#
# Replace with a different usecase scenario if necessary
# Usecase scenarios are named after a folder names under 'usecase' directory
export USECASE=${USECASE:-"sriovnet_rdma"}
#export USECASE=${USECASE:-"hostdev_rdma_sriov"}
#export USECASE=${USECASE:-"ipoib_rdma_shared_device"}
#export USECASE=${USECASE:-"macvlan_rdma_shared_device"}
#export USECASE=${USECASE:-"sriovibnet_rdma"}

# default number of VFs to define
case ${USECASE} in
hostdev_rdma_sriov|sriovibnet_rdma|sriovnet_rdma)
  export NUM_VFS=${NUM_VFS:-"8"}
  ;;
ipoib_rdma_shared_device|macvlan_rdma_shared_device)
  export NUM_VFS=${NUM_VFS:-"0"}
  ;;
esac
if [ ! -v DEVICE_TYPES ];then
DEVICE_TYPES=( "connectx-6" )
fi
#
# select the ipam mode.
# nv-ipam: preferred scaleable > 1000's
# whereabouts small clusters < 60 nodes
#

# https://github.com/Mellanox/nvidia-k8s-ipam
export IPAM_TYPE=${IPAM_TYPE:-"nv-ipam"}
#export NVIPAM_POOL_TYPE="CIDRPool"
export NVIPAM_POOL_TYPE=${NVIPAM_POOL_TYPE:-"IPPool"}
#export IPAM_TYPE="whereabouts"
#export IPAM_TYPE="dhcp"

#
# set to false when running host mofed, not container version
#
OFED_ENABLE=${OFED_ENABLE:-"true"}
#
# set true to fix OFED_BLACKLIST bug
#
OFED_BLACKLIST_ENABLE=${OFED_BLACKLIST_ENABLE:-"false"}
OFED_BLACKLIST_MODULES=${OFED_BLACKLIST_ADD:-""}
#
# disable when GPU-operator is running NFD
#
NFD_ENABLE=${NFD_ENABLE:-"true"}
#
# set true when only want the config yaml files created
#
CREATE_CONFIG_ONLY=${CREATE_CONFIG_ONLY:-"1"}

if [ "${CREATE_CONFIG_ONLY}" = "1" ];then
  docmd="echo Install command: "
else
  docmd=""
fi
#
# for RDMA AI E/W traffic, 9000 is usally the selected MTU number
#
NETOP_MTU=${MTU_DEFAULT:-"1500"}
# Use shared or exclusive mode for rdma namespace.
# When set to false, only allocated RDMA devices will be visible on the pod
# When set to true, all RDMA devices will be visible on the pod
RDMASHAREDMODE=${RDMASHAREDMODE:-"true"}
#
# use when setting Source Based Routing
# loads the SBR plugin for the network definition
#
SBRMODE=${SBRMODE:-"false"}
#
# default enable NIC_CONFIG_OPERATOR
#
export NIC_CONFIG_ENABLE=${NIC_CONFIG_ENABLE:-"true"}
#
# default enable NFSRDMA
#
export ENABLE_NFSRDMA=${ENABLE_NFSRDMA:-"false"}
#
# node selector default for node-role.kubernetes.io
#
export WORKERNODE=${WORKERNODE:-"worker"}
#
# feature gates
#
export FG_PARALLEL_NIC_CONFIG=${FG_PARALLEL_NIC_CONFIG:-"true"}
export FG_RESOURCE_INJECTOR_MATCH=${FG_RESOURCE_INJECTOR_MATCH:-"false"}
export METRICS_EXPORTER=${METRICS_EXPORTER:-"false"}
export MANAGE_SW_BRIDGE=${MANAGE_SW_BRIDGE:-"false"}
export FG_MLNX_FW_RESET=${FG_MLNX_FW_RESET:-"false"}
source ${NETOP_ROOT_DIR}/usecase/${USECASE}/netop.cfg
